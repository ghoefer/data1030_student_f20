{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[42m[ OK ]\u001b[0m Python version is 3.7.4 (v3.7.4:e09359112e, Jul  8 2019, 14:54:52) \n",
      "[Clang 6.0 (clang-600.0.57)]\n",
      "\n",
      "\u001b[42m[ OK ]\u001b[0m numpy version 1.18.5 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m matplotlib version 3.2.2 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m sklearn version 0.23.1 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m pandas version 1.0.5 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m xgboost version 1.1.1 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m shap version 0.35.0 is installed.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from distutils.version import LooseVersion as Version\n",
    "import sys\n",
    "\n",
    "OK = '\\x1b[42m[ OK ]\\x1b[0m'\n",
    "FAIL = \"\\x1b[41m[FAIL]\\x1b[0m\"\n",
    "\n",
    "try:\n",
    "    import importlib\n",
    "except ImportError:\n",
    "    print(FAIL, \"Python version 3.7 is required,\"\n",
    "                \" but %s is installed.\" % sys.version)\n",
    "def import_version(pkg, min_ver, fail_msg=\"\"):\n",
    "    mod = None\n",
    "    try:\n",
    "        mod = importlib.import_module(pkg)\n",
    "        if pkg in {'PIL'}:\n",
    "            ver = mod.VERSION\n",
    "        else:\n",
    "            ver = mod.__version__\n",
    "        if Version(ver) == min_ver:\n",
    "            print(OK, \"%s version %s is installed.\"\n",
    "                % (lib, min_ver))\n",
    "        else:\n",
    "            print(FAIL, \"%s version %s is required, but %s installed.\"\n",
    "                % (lib, min_ver, ver))\n",
    "    except ImportError:\n",
    "        print(FAIL, '%s not installed. %s' % (pkg, fail_msg))\n",
    "    return mod\n",
    "\n",
    "# first check the python version\n",
    "pyversion = Version(sys.version)\n",
    "if pyversion >= \"3.7\":\n",
    "    print(OK, \"Python version is %s\" % sys.version)\n",
    "elif pyversion < \"3.7\":\n",
    "    print(FAIL, \"Python version 3.7 is required,\"\n",
    "                \" but %s is installed.\" % sys.version)\n",
    "else:\n",
    "    print(FAIL, \"Unknown Python version: %s\" % sys.version)\n",
    "\n",
    "    \n",
    "print()\n",
    "requirements = {'numpy': \"1.18.5\", 'matplotlib': \"3.2.2\",'sklearn': \"0.23.1\",\n",
    "                'pandas': \"1.0.5\",'xgboost': \"1.1.1\", 'shap': \"0.35.0\"}\n",
    "# now the dependencies\n",
    "for lib, required_version in list(requirements.items()):\n",
    "    import_version(lib, required_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questions:\n",
      "\n",
      "1. Is the dataset IID or not?\n",
      "The dataset is IID because the data does not depend on each other. It has no memory of past samples.\n",
      "Each row is not related to each other.\n",
      "\n",
      "2. Please decide what fraction of points will be in each set and explain your decision.\n",
      "answer\n",
      "Its usually safe to do 60% train, 20% validation, 20% test for small datasets.\n",
      "\n",
      "3. Please explain in a paragraph or two why it is important to fit the preprocessors on the training set only.\n",
      "Data very rarely comes in the proper format for machine learning. Often it contains many strings that are not numerical values. You need to standardize all of the features so the mean=0, stdev=1. Difficult to do analysis when data is not normalized yet.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AGE</th>\n",
       "      <th>SEX</th>\n",
       "      <th>BMI</th>\n",
       "      <th>BP</th>\n",
       "      <th>S1</th>\n",
       "      <th>S2</th>\n",
       "      <th>S3</th>\n",
       "      <th>S4</th>\n",
       "      <th>S5</th>\n",
       "      <th>S6</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59</td>\n",
       "      <td>2</td>\n",
       "      <td>32.1</td>\n",
       "      <td>101.0</td>\n",
       "      <td>157</td>\n",
       "      <td>93.2</td>\n",
       "      <td>38.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.8598</td>\n",
       "      <td>87</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>21.6</td>\n",
       "      <td>87.0</td>\n",
       "      <td>183</td>\n",
       "      <td>103.2</td>\n",
       "      <td>70.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.8918</td>\n",
       "      <td>69</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>72</td>\n",
       "      <td>2</td>\n",
       "      <td>30.5</td>\n",
       "      <td>93.0</td>\n",
       "      <td>156</td>\n",
       "      <td>93.6</td>\n",
       "      <td>41.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.6728</td>\n",
       "      <td>85</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>25.3</td>\n",
       "      <td>84.0</td>\n",
       "      <td>198</td>\n",
       "      <td>131.4</td>\n",
       "      <td>40.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.8903</td>\n",
       "      <td>89</td>\n",
       "      <td>206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>23.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>192</td>\n",
       "      <td>125.4</td>\n",
       "      <td>52.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.2905</td>\n",
       "      <td>80</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AGE  SEX   BMI     BP   S1     S2    S3   S4      S5  S6    Y\n",
       "0   59    2  32.1  101.0  157   93.2  38.0  4.0  4.8598  87  151\n",
       "1   48    1  21.6   87.0  183  103.2  70.0  3.0  3.8918  69   75\n",
       "2   72    2  30.5   93.0  156   93.6  41.0  4.0  4.6728  85  141\n",
       "3   24    1  25.3   84.0  198  131.4  40.0  5.0  4.8903  89  206\n",
       "4   50    1  23.0  101.0  192  125.4  52.0  4.0  4.2905  80  135"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading file\n",
    "# sep = '\\s+'  means any amount of white space as delimeter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# df = pd.read_csv(\"diabetes-data.csv\")\n",
    "df =  pd.read_csv('diabetes-data.csv', sep='\\s+'  , engine='python')\n",
    "# ids = df[\"ResponseId\"]\n",
    "\n",
    "print('Questions:')\n",
    "print()\n",
    "print('1. Is the dataset IID or not?')\n",
    "# IID: Independent and Identically Distributed\n",
    "    # all samples come from same generative process and have no memory of past generated samples\n",
    "            # identify cats and dogs on images\n",
    "            # predict the house price\n",
    "            # predict if someone's salary is above or below 50k\n",
    "# non-IID:\n",
    "    #data generated by time-dependent processes\n",
    "            # data has group structure \n",
    "            # (samples collected from e.g., different subjects, experiments, measurement devices)\n",
    "            # stock price, youtube video stats, returning customer spending\n",
    "            # multiple data points that belong to the same subject or same group\n",
    "\n",
    "print('The dataset is IID because the data does not depend on each other. It has no memory of past samples.')\n",
    "print('Each row is not related to each other.')\n",
    "print()\n",
    "print('2. Please decide what fraction of points will be in each set and explain your decision.')\n",
    "print('answer')\n",
    "print('Its usually safe to do 60% train, 20% validation, 20% test for small datasets.')\n",
    "print()\n",
    "print('3. Please explain in a paragraph or two why it is important to fit the preprocessors on the training set only.')\n",
    "print('Data very rarely comes in the proper format for machine learning. ' \n",
    "      'Often it contains many strings that are not numerical values.'\n",
    "      ' You need to standardize all of the features so the mean=0, stdev=1.'\n",
    "        ' Difficult to do analysis when data is not normalized yet.')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set: (265, 10) (265,)\n",
      "(177, 10) (177,)\n",
      "validation set: (88, 10) (88,)\n",
      "test set: (89, 10) (89,)\n"
     ]
    }
   ],
   "source": [
    "# Basic Split (Train-test)\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "\n",
    "# help(train_test_split)\n",
    "\n",
    "random_state = 42\n",
    "\n",
    "# let's separate the feature matrix X, and target variable y\n",
    "y = df['BMI'] # remember, we want to predict BMI\n",
    "X = df.loc[:, df.columns != 'BMI'] # all other columns are features\n",
    "# print(y)\n",
    "# print(X.head())\n",
    "\n",
    "# first split to separate out the training set\n",
    "# 60% train\n",
    "X_train, X_other, y_train, y_other = train_test_split(X,y,train_size = 0.6,random_state=random_state)\n",
    "print('training set:',X_train.shape, y_train.shape) # 60% of points are in train\n",
    "print(X_other.shape, y_other.shape) # 40% of points are in other\n",
    "\n",
    "# second split to separate out the validation and test sets\n",
    "# 40% test; (20% validation 20% test)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_other,y_other,train_size = 0.5,random_state=random_state)\n",
    "print('validation set:',X_val.shape, y_val.shape) # 20% of points are in validation\n",
    "print('test set:',X_test.shape, y_test.shape) # 20% of points are in test\n",
    "\n",
    "# IMPORTANT: RANDOMNESS\n",
    "# there is always a randomness in splitting, depending on which points are in train validation vs. tests\n",
    "# this is a constant noise that occurs\n",
    "# as a result, important to run this a few times and calculate the mean + stdev of the test scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AGE</th>\n",
       "      <th>SEX</th>\n",
       "      <th>BMI</th>\n",
       "      <th>BP</th>\n",
       "      <th>S1</th>\n",
       "      <th>S2</th>\n",
       "      <th>S3</th>\n",
       "      <th>S4</th>\n",
       "      <th>S5</th>\n",
       "      <th>S6</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59</td>\n",
       "      <td>2</td>\n",
       "      <td>32.1</td>\n",
       "      <td>101.0</td>\n",
       "      <td>157</td>\n",
       "      <td>93.2</td>\n",
       "      <td>38.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.8598</td>\n",
       "      <td>87</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>21.6</td>\n",
       "      <td>87.0</td>\n",
       "      <td>183</td>\n",
       "      <td>103.2</td>\n",
       "      <td>70.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.8918</td>\n",
       "      <td>69</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>72</td>\n",
       "      <td>2</td>\n",
       "      <td>30.5</td>\n",
       "      <td>93.0</td>\n",
       "      <td>156</td>\n",
       "      <td>93.6</td>\n",
       "      <td>41.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.6728</td>\n",
       "      <td>85</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>25.3</td>\n",
       "      <td>84.0</td>\n",
       "      <td>198</td>\n",
       "      <td>131.4</td>\n",
       "      <td>40.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.8903</td>\n",
       "      <td>89</td>\n",
       "      <td>206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>23.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>192</td>\n",
       "      <td>125.4</td>\n",
       "      <td>52.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.2905</td>\n",
       "      <td>80</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AGE  SEX   BMI     BP   S1     S2    S3   S4      S5  S6    Y\n",
       "0   59    2  32.1  101.0  157   93.2  38.0  4.0  4.8598  87  151\n",
       "1   48    1  21.6   87.0  183  103.2  70.0  3.0  3.8918  69   75\n",
       "2   72    2  30.5   93.0  156   93.6  41.0  4.0  4.6728  85  141\n",
       "3   24    1  25.3   84.0  198  131.4  40.0  5.0  4.8903  89  206\n",
       "4   50    1  23.0  101.0  192  125.4  52.0  4.0  4.2905  80  135"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Preprocessing\n",
    "    # Categorical: use ordinal or one-hot encoding\n",
    "    # Continuous: Scaling and normalization\n",
    "\n",
    "# STEP 1: Use transformers\n",
    "    \n",
    "    # All transformes have three methods:\n",
    "        # fit method: estimates parameters necessary to do the transformation,\n",
    "        # transform method: transforms the data based on the estimated parameters,\n",
    "        # fit_transform method: both steps are performed at once, this can be faster than doing the steps separately.\n",
    "        \n",
    "    # Transformers we cover in class:\n",
    "        # OneHotEncoder - converts categorical features into dummy arrays\n",
    "        # OrdinalEncoder - converts categorical features into an integer array\n",
    "        # MinMaxScaler - scales continuous variables to be between 0 and 1\n",
    "        # StandardScaler - standardizes continuous features by removing the mean and scaling to unit variance\n",
    "        # LabelEncoder - converts text target variable to numerical values\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(['HS-grad', 'College', 'Bachelors', 'Masters', 'Doctorate'],\n",
      "      dtype=object)]\n",
      "transformed train features: \n",
      "[[2.]\n",
      " [3.]\n",
      " [2.]\n",
      " [4.]\n",
      " [0.]\n",
      " [3.]]\n",
      "transformed test features: \n",
      "[[0.]\n",
      " [3.]\n",
      " [3.]\n",
      " [1.]\n",
      " [2.]]\n"
     ]
    }
   ],
   "source": [
    "# Ordinal Encoding\n",
    "    # Use in features where categories can be ranked or ordered\n",
    "    # Education level, reaction \"severe\", \"response\", \"excellent\"\n",
    "    \n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "\n",
    "# example data\n",
    "train_edu = {'educational level':['Bachelors','Masters','Bachelors','Doctorate','HS-grad','Masters']} \n",
    "test_edu = {'educational level':['HS-grad','Masters','Masters','College','Bachelors']}\n",
    "\n",
    "Xtoy_train = pd.DataFrame(train_edu)\n",
    "Xtoy_test = pd.DataFrame(test_edu)\n",
    "\n",
    "\n",
    "# initialize the encoder -- manual ordered list of categories\n",
    "# the string 'HS-grad' will be replaced by 0, Doctorate will be #4\n",
    "cats = [['HS-grad','College','Bachelors','Masters','Doctorate']]\n",
    "enc = OrdinalEncoder(categories = cats) # The ordered list of \n",
    "# categories need to be provided. By default, the categories are alphabetically ordered!\n",
    "\n",
    "\n",
    "# fit the training data\n",
    "enc.fit(Xtoy_train)\n",
    "print(enc.categories_)\n",
    "\n",
    "# transform X_train and X_test\n",
    "# We could have used enc.fit_transform(X_train) to combine fit and transform\n",
    "X_train_oe = enc.transform(Xtoy_train)\n",
    "print('transformed train features: ')\n",
    "print(X_train_oe)\n",
    "X_test_oe = enc.transform(Xtoy_test) # OrdinalEncoder error if it encounters an unknown category in test\n",
    "print('transformed test features: ')\n",
    "print(X_test_oe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categories: [array(['Female', 'Male', 'Unknown'], dtype=object), array(['Chrome', 'Internet Explorer', 'Safari'], dtype=object)]\n",
      "feature names: ['gender_Female' 'gender_Male' 'gender_Unknown' 'browser_Chrome'\n",
      " 'browser_Internet Explorer' 'browser_Safari']\n",
      "X_train transformed\n",
      "[[0. 1. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 1. 0.]\n",
      " [0. 1. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 1. 0.]]\n",
      "X_test transformed\n",
      "[[1. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# One hot Encoder -- ordinal\n",
    "    # use if categories cannot be clearly ordered (gender, workclass, relationship status)\n",
    "    # converts 1 categorical column into multiple columns\n",
    "    # if feature has X category, then it's 1. otherwise it's 0. (Female =1, male=0, other=0)\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# help(OneHotEncoder)\n",
    "\n",
    "# toy example\n",
    "train = {'gender':['Male','Female','Unknown','Male','Female','Female'],\\\n",
    "         'browser':['Safari','Safari','Internet Explorer','Chrome','Chrome','Internet Explorer']}\n",
    "test = {'gender':['Female','Male','Unknown','Female'],'browser':['Chrome','Firefox','Internet Explorer','Safari']}\n",
    "\n",
    "Xtoy_train = pd.DataFrame(train)\n",
    "Xtoy_test = pd.DataFrame(test)\n",
    "\n",
    "ftrs = ['gender','browser']\n",
    "\n",
    "\n",
    "# initialize the encoder\n",
    "enc = OneHotEncoder(sparse=False,handle_unknown='ignore') # by default, OneHotEncoder returns a sparse matrix. \n",
    "                                                          # sparse=False returns a 2D array\n",
    "\n",
    "# fit the training data\n",
    "enc.fit(Xtoy_train)\n",
    "print('categories:',enc.categories_)\n",
    "print('feature names:',enc.get_feature_names(ftrs))\n",
    "\n",
    "# transform X_train\n",
    "X_train_ohe = enc.transform(Xtoy_train)\n",
    "#print(X_train_ohe)\n",
    "\n",
    "# do all of this in one step\n",
    "X_train_ohe = enc.fit_transform(Xtoy_train)\n",
    "print('X_train transformed')\n",
    "print(X_train_ohe)\n",
    "\n",
    "# transform X_test\n",
    "X_test_ohe = enc.transform(Xtoy_test)\n",
    "print('X_test transformed')\n",
    "print(X_test_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformed data:\n",
      "[[0.30645161 0.        ]\n",
      " [0.83870968 0.66666667]\n",
      " [0.         0.16666667]\n",
      " [0.88709677 1.        ]\n",
      " [0.46774194 0.66666667]\n",
      " [1.         0.33333333]\n",
      " [0.30645161 0.66666667]]\n",
      "\n",
      "train data:\n",
      "[[ 1.12903226  0.        ]\n",
      " [ 0.20967742  0.66666667]\n",
      " [-0.0483871   0.        ]\n",
      " [ 0.75806452  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Min Max Scaler -- continuous\n",
    "    # Continuous features that are reasonably bounded (e.g., age (0-100), # hrs worked per week (0-80))\n",
    "    # Plot the histogram feature to verify if needed\n",
    "    # do not refit Min Max Scaler on test set\n",
    "    \n",
    "\n",
    "# numbers are automatically put between 0 and 1\n",
    "# this is automatically done in the minmaxscaler; it does it like this:\n",
    "# (value - min) / (max - min), if value is 32, min is 13 and max is 75, then we have 19 / 62 = 0.3064\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# help(MinMaxScaler)\n",
    "\n",
    "\n",
    "# toy data\n",
    "# let's assume we have two continuous features:\n",
    "train = {'age':[32,65,13,68,42,75,32],'number of hours worked':[0,40,10,60,40,20,40]}\n",
    "test = {'age':[83,26,10,60],'number of hours worked':[0,40,0,60]}\n",
    "\n",
    "\n",
    "Xtoy_train = pd.DataFrame(train)\n",
    "Xtoy_test = pd.DataFrame(test)\n",
    "\n",
    "# initialize and fit the scaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(Xtoy_train)\n",
    "print('transformed data:')\n",
    "print(scaler.transform(Xtoy_train))\n",
    "print()\n",
    "print('train data:')\n",
    "print(scaler.transform(Xtoy_test)) # note how scaled X_test contains values larger than 1 and smaller than 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data: \n",
      "[[-0.44873188]\n",
      " [-0.36895732]\n",
      " [-0.4806417 ]\n",
      " [ 2.58270127]\n",
      " [-0.51255153]\n",
      " [ 0.18946457]\n",
      " [-0.49659661]\n",
      " [-0.46468679]]\n",
      "\n",
      "test data: \n",
      "[[-0.52850644]\n",
      " [-0.43277697]\n",
      " [ 4.1781924 ]\n",
      " [-0.41682206]]\n"
     ]
    }
   ],
   "source": [
    "# Standard Scaler -- continuous\n",
    "    # Use if continuous feature follows and tailed distribution\n",
    "    # E.g., salaries (most people earn <100k but there are some super rich people)\n",
    "    # mean =0, stdev=1\n",
    "    \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# help(StandardScaler)\n",
    "\n",
    "# toy data\n",
    "train = {'salary':[50_000,75_000,40_000,1_000_000,30_000,250_000,35_000,45_000]}\n",
    "test = {'salary':[25_000,55_000,1_500_000,60_000]}\n",
    "\n",
    "Xtoy_train = pd.DataFrame(train)\n",
    "Xtoy_test = pd.DataFrame(test)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "print('train data: ')\n",
    "print(scaler.fit_transform(Xtoy_train))\n",
    "print()\n",
    "print('test data: ')\n",
    "print(scaler.transform(Xtoy_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data:\n",
      "[1 1 1 0 0 1]\n",
      "\n",
      "test data:\n",
      "[0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "# Label Encoder\n",
    "    # Classification labels need to be integers with values between 0 and n_classes-1\n",
    "    # Labels should be 0, 1 or if there are 3 classes, then 0, 1, 2.\n",
    "    \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# help(LabelEncoder)\n",
    "\n",
    "# toy data\n",
    "ytoy_train = ['>50K','>50K', '>50K', '<=50K', '<=50K', '>50K']\n",
    "ytoy_test = ['<=50K','>50K','>50K','<=50K']\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "print('train data:')\n",
    "print(le.fit_transform(ytoy_train))\n",
    "print()\n",
    "print('test data:')\n",
    "print(le.transform(ytoy_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How and when to do preprocessing\n",
    "    # Apply transformer.fit ONLY on your TRAINING data.\n",
    "    # **Fit on the training set, then transform on validation and test sets.**\n",
    "    # Fit should not go on the entire dataset because it will impact how the training and validation sets are transformed\n",
    "        # only fit on training set once\n",
    "        # test set might include outliers, so if you fit this too it will influence it\n",
    "        # test set always needs to be separate from the train and validation sets\n",
    "        \n",
    "    # do not apply fit_transform on all three, separately\n",
    "        # The relative position of the points will change because the min max on test will be different from the min\n",
    "        # max on the training set\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
